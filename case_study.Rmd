---
title: "Fast and reliable use of numerical solvers"
date: "`r Sys.Date()`"
author: "Juho Timonen and Ben Bales"
link-citations: true
output:
  html_document:
    number_sections: true
    toc: true
    toc_float: true
    toc_depth: 4
    highlight: pygments
    theme: cosmo
    css: "style.css"
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load, message=FALSE}
# Requirements
require(rstan)
library(tidyverse)
require(bayesplot)
require(grDevices)
require(ggplot2)
require(loo)
require(stats)
require(posterior)
set.seed(123)
```

The required `posterior` package is not in CRAN but can be installed via 
`install.packages("posterior", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))`.

# Introduction

Assume we have a Bayesian model that involves a numeric solution to some ODE or
PDE (ordinary or partial differential equation), which contains
uncertain parameters. Try as we might, these calculations are often expensive
and frequently become the limiting factors in whatever model they are involved
in. The simplest things we might do to speed up our calculations are lowering
the timestep, coarsening the discretization, or increasing the tolerance of the
solvers.

That immediately leaves us with the question, is this okay? Has loosening the
tolerance affected the statistics of our solution? The technical answer is yes,
the practical answer is maybe, and the ideal case is no, but without checking
there's no way of knowing.

How can something like this be checked? The numerical approximation
answer is we should compare to a reference model and verify that the solution
at all points in space/time in the approximation are appropriately close
to the reference. That isn't so much of a problem in and of itself, but we're
doing statistics, and so we need to know that the solution is accurate enough
across all relevant parts of parameter space. Additionally, the "relevant parts
of parameter space" is the posterior itself which is not known beforehand!

The problem of validating an approximation in a Bayesian model is
significantly more complicated than in the classical numerical analysis world.
The point of this case study is to show how by adding one additional tool,
namely Pareto-Smoothed Importance Sampling (PSIS; @yao2018, @vehtari2019), we
can solve this problem.

# Workflow

Assume we have two models, $M_{low}$ and $M_{high}$.
These are otherwise the same model, except that $M_{low}$ uses a cheaper and
less accurate numerical methods (or just looser tolerances) to compute the
required ODE or PDE solutions. Therefore they have different posteriors,
$p_{low}$ and $p_{high}$, respectively.

To understand how PSIS comes into play, we must first discuss importance
sampling. If we want to compute expectations with the high precision model, we
can take draws from the low precision models and reweight these according to the
importance weights $\frac{p_{high}}{p_{low}}$. If these models are too
different, then the reweighting will produce noisy estimates that are not
useful. PSIS and particularly the $\hat{k}$ diagnostic (pronounced k-hat) is
the tool that tells us when we can or cannot rely on the importance weights. If
$\hat{k} < 0.5$ we are safe to do the importance sampling, if $\hat{k} < 0.7$
the importance sampling will start to converge more slowly, and if
$\hat{k} > 0.7$ the importance sampling estimates are unreliable. For simplicity
we will only consider the $\hat{k} < 0.5$ threshold.

Ideally, $M_{high}$ would involve a numerical method that we can trust
completely in all parts of the parameter space so that, as long as
$\hat{k} < 0.5$, importance weights can be used to reweight the low precision
approximation $p_{low}$ to the high precision approximation $p_{high}$. We can
thik of $M_{high}$ as a reference model, because it is the baseline to which
we compare. It is difficult in practice to have a reference model that works
in all parts of parameter space because the properties (such as stiffness) of an
ODE system can be very different in different parts of the parameter space.
Classically accuracy can be checked at a given set of parameters fairly easily,
but not over a high dimensional parameter space. Under these conditions it is
necessary to compromise to develop a reference model that works only over a
range of parameter space, but even then it is hard to know a-priori what range
that is.

We propose a workflow which addresses not only the problem of picking an
appropriate approximation but also the problem of developing a good reference
model.

::: {#workflow .box}
1. Generate draws from $p_{low}$.
2. Tune the numerical method in $M_{high}$ so that it is reliable at
these draws. All application specific knowledge and classical numerical
analysis can be used here.
3. Compute importance weights $\frac{p_{high}}{p_{low}}$
4. Compute the $\hat{k}$ diagnostic. If $\hat{k} > 0.5$, raise precision of $p_{low}$ and go back to step 1.
5. Resample draws using importance weights.
:::

The next two sections of this case study outline how to apply this workflow to:

1. Work with a low-precision ODE model
2. Use a PDE solver that does not have explicit tolerance controls

The importance sampling diagnostics are handled with the
[`loo`](https://mc-stan.org/loo) package and the resampling is handled with the
[`posterior`](https://github.com/stan-dev/posterior) package.

# SIR example (ODE)

Here we study a simple Susceptible-Infected-Recovered (SIR) model of disease
spread. This model is described in more detail in
[this case study](https://mc-stan.org/users/documentation/case-studies/boarding_school_case_study.html).

The states of the ODE are amounts of susceptible (S), infected (I) and recovered
(R) people. The dynamics are given by the ODE system:

\begin{align}
    \frac{dS}{dt} &= -\beta \cdot I \cdot \frac{S}{N} \\
    \frac{dI}{dt} &=  \beta  \cdot I \cdot \frac{S}{N} - \gamma \cdot I \\
    \frac{dR}{dt} &=  \gamma \cdot I
\end{align}

The parameters $\beta, \gamma > 0$ can be estimated from counts of the number
of infected people (I).

First, we will import a function from Stan for solving the ODE.

```{r, message=FALSE, results=FALSE}
model = stan_model("stan/sir.stan")
expose_stan_functions(model)
```

Second, we define various useful constants for our problem (the initial
conditions, etc.) and a wrapper function for the Stan function that solves
the ODE. The actual function exported from Stan is a bit awkward so we rewrap
it here in a way that the only options we will expose are parameters and
tolerance arguments.

```{r}
N = 16                                # number of days measured
M = 1000                              # population size
initial_infected = 20                 # number of infected on day 0
initial_conditions = c(M - I0, I0, 0) # S, I, R on day 0
ts = seq_len(n_days)                  # measurement times
theta_true = c(1, 0.2)                # true parameter values

# Solve the SIR system and format result array
# - theta = [beta, gamma]
solve_sir = function(theta, rtol, atol) {
  stan_solve_sir(initial_conditions, ts, theta,
                 c(0.0), M, rtol, atol) %>%
    unlist %>%
    matrix(ncol = 3, byrow = TRUE)
}
```

The two tolerance arguments are the relative tolerance, `rtol`, and the
absolute tolerance, `atol`. Roughly speaking, the `atol` is the tolerance for
numbers near zero (the solver will be able to resolve numbers down to about
the scale of `atol`) and `rtol` is the tolerance on the relative error the
solver can make away from zero.

We can just pick a couple tolerances and quickly run and plot a solution
just to get a feel for what the system looks like:

```{r}
# Function for plotting a solution y_hat
plot_sir = function(y0, y_hat) {
  yyy = rbind(y0, y_hat)
  ttt = c(0.0, ts)
  y = as.vector(yyy)
  day = rep(ttt, 3)
  comp = rep( c("S", "I", "R"), each = length(ttt))
  df = data.frame(day, y, as.factor(comp))
  colnames(df) = c("Day", "y", "Compartment")
  aes = aes_string(x = "Day", y = "y", group = "Compartment",
                    color = "Compartment")
  plt = ggplot(df, aes) + geom_line(lwd = 1) + geom_point()
  return(plt)
}

plot_sir(initial_conditions, solve_sir(theta_true, 1e-4, 1e-4))
```

In this model we are making noisy measurements of the number of infected
people (I) at each day.

## Generating data

If we're going to generate data from our model we better have an accurate ODE
solver, otherwise we're just generating data from some weird approximate model.

The simplest way to check that an `atol` and `rtol` are suitable are to do
a solve at one tolerance level, repeat the solve at a much smaller (more
precise) tolerance, and then look at the maximum absolute error at any output
point. We will create a function to do this automatically:

```{r}
check_reliability = function(theta, rtol, atol) {
  y_hat = solve_sir(theta, rtol, atol)
  y_hat2 = solve_sir(theta, rtol / 10, atol / 10)
  max_abs_err = max(abs(y_hat - y_hat2))
  return(max_abs_err)
}
```

We can study the maximum absolute error compared to a solution with 10 smaller
tolerances, as a function `tol = atol = rtol`.

```{r, fig.width=7, fig.height=3.5}
errors = c()
tols = 10^(-c(1:12))
for (tol in tols) {
  errors = c(errors, check_reliability(theta_true, tol, tol))
}

qplot(tols, errors, geom = c("point", "line")) +
  scale_x_log10() +
  scale_y_log10()
```

We decide that $10^{-6}$ is a good enough value to use for `atol` and `rtol`
during simulation. Certainly we do not expect to be measuring patients in the
hospital down to an accuracy of $10^{-4}$ patients.

We add negative binomial noise to get an observed number of infected people
(cases) at each time point.

```{r, fig.width=7, fig.height=4.8}
atol =  1e-6
rtol = 1e-6

dispersion = 5 # noise parameter for negative binomial
mu = solve_sir(theta_true, atol, rtol)[, 2]
y = stats::rnbinom(length(ts), mu = mu, size = dispersion)

tibble(t = ts, mu = mu, y = y) %>%
  ggplot() +
  geom_line(aes(t, mu)) +
  geom_point(aes(t, y)) +
  xlab("Day") +
  ylab("Infected people") +
  ggtitle("Simulated data as points,\n Underlying solution as lines")
```

## Applying the workflow

As a reminder, our mission in fitting this ODE is to use a low precision
solver. It is always tempting to use low precision solvers when working
with ODEs because they run faster. The difficulty becomes how to deal with
the approximation. Does the approximation matter? If so, how can it be
corrected? These are the questions the workflow here will allow us to answer.

### Generating draws from $p_{low}$

The first step in the workflow is to take an approximation and fit
the data. In this case, we will use `rtol = 1e-4`, `atol = 1e-3`, which are
quite low tolerances in general.

```{r, fig.width=7, fig.height=4.5, results=FALSE}
rtol_low = 1e-4
atol_low = 1e-3

fit = rstan::sampling(model, list(
  N = length(ts),
  M = M,
  ts = ts,
  y = y,
  initial_conditions = initial_conditions,
  rtol = rtol_low,
  atol = atol_low))
```


```{r}
print(get_elapsed_time(fit))
print(fit, pars = c("beta", "gamma", "phi"))
```

### Tuning the reference method

Before we can diagnose if this approximate posterior can be corrected to
something more reliable, we need to develop that more reliable method.

In this case, `rtol = atol = 1e-6` was accurate enough generating the data,
so let's check if it's accurate in all the draws in this posterior.

```{r}
atol_high = 1e-6
rtol_high = 1e-6
draws = rstan::extract(fit, pars = c("beta", "gamma", "phi"))
phi_draws = draws$phi
theta_draws = cbind(draws$beta, draws$gamma)
num_draws = length(phi_draws)

# Compute differences
errors = c()
for (i in 1:num_draws) {
  mae = check_reliability(theta_draws[i,], atol_high, rtol_high)
  errors = c(errors, mae)
}
```

The error for each draw shows that overall we are keeping under a maximum
absolute error of one milliperson. This seems accurate enough.

```{r}
qplot(errors, geom = "histogram")
```

### Computing importance weights

With the reference model in place, it is time to compute the importance
weights $\frac{p_{high}}{p_{low}}$. On paper this is relatively simple,
just compute the log density of the reference model and the log
density of the low precision model and take the difference (we work
with the log of the importance ratios $\log p_{high} - \log p_{low}$
for numeric stability).

The downside to these importance weights is that it might take some
time to compute the log densities of the reference model. The
upside is that all the calculations can be done in parallel, and it should
still be way faster than sampling with the reference model itself.

If the priors are kept the same between the reference and low precision
model, then those can be left out of this calculation (they will cancel).

```{r}
log_lh_low = rep(0, num_draws)
log_lh_high = rep(0, num_draws)
for (i in seq_len(num_draws)) {
  y_hat_low = solve_sir(theta_draws[i,], rtol_low, atol_low)
  y_hat_high = solve_sir(theta_draws[i,], rtol_high, atol_high)
  log_lh_low[i] = sum(dnbinom(y, size = phi_draws[i],
                              mu = y_hat_low[, 2], log = TRUE))
  log_lh_high[i] = sum(dnbinom(y, size = phi_draws[i],
                              mu = y_hat_high[, 2], log = TRUE))
}
log_weights = log_lh_high - log_lh_low
```

We can plot the log importance ratios and see they are all close to zero
(which is good).

```{r}
qplot(log_weights, geom = "histogram")
```

### Computing $\hat{k}$ diagnostic

With the importance ratios calculated, we can check if they are usable or not
with the PSIS $\hat{k}$ diagnostic.

```{r pareto_sir, fig.width=6, fig.height=4.5}
loo::psis(log_weights)
```

$\hat{k} < 0.5$, and so it is possible to use importance sampling to correct
this approximation.

### Resampling

At this point we have a weighted set of posterior draws. It
is usually easier to work with a set of draws than a set of weighted draws, so
we can resample our weighted draws using `posterior::resample_draws`.

Just because it is possible to do an importance sampling correction on a set
of draws does not mean that unweighted statistics on these draws are safe
to use. In this case, the results are not much different, but it is relatively
trivial to do a resampling with the `posterior` package:

```{r}
w = exp(log_weights)
draws_list = rstan::extract(fit, c("beta", "gamma"))
draws_df = posterior::as_draws_df(draws_list)
resampled_df = posterior::resample_draws(draws_df,
                                         weights = w)

print(draws_df %>% posterior::summarize_draws())
print(resampled_df %>% posterior::summarize_draws())
```

<!-- WEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE -->

# Heat Equation Example (PDE)

## Problem definition
In this example we consider the diffusion of heat ($u(t, x)$) in a rod
($x \in [0, L]$). In this hypothetical experiment, the rod is cooled to room
temperature and then heated from the left side. After some time the temperature
profile of the rod is measured and from this the thermal diffusivity is to be
estimated.

The dynamics are governed by the 1D heat equation:

\begin{align}
\frac{\partial u}{\partial t} &= K \cdot \frac{\partial^2 u}{\partial x^2} \\
u(0, x) &= 0 \\
u(t, 0) &= 1 \\
u(t, L) &= 0
\end{align}


## Solving the system
All of the computations in this example are going to be done with a method of
lines discretization of this problem and a backwards Euler integrator. The
appropriate math is described in the online lecture notes
[ATM 623: Climate Modeling](http://www.atmos.albany.edu/facstaff/brose/classes/ATM623_Spring2015/Notes/Lectures/Lecture16%20--%20Numerical%20methods%20for%20diffusion%20models.html) by Brian E. J. Rose, though any introductory PDE reference should suffice.

For convenience we have defined a Stan function that solve our problem
given a timestep, a spatial discretization, a hypothetical diffusivity, a
measurement time, and a list of measurement points and returns the predicted
temperature at each of those measurement points.

```{r}
model = stan_model("stan/diffusion.stan")
expose_stan_functions(model)
```

```{r}
dt = 1.0
Nx = 10
K = 1e-1
T_meas = 0.1
x_meas = c(-1.0, 0.01, 0.5, 0.99, 1.0, 2.0)

solve_pde(dt, Nx, K, T_meas, x_meas)
```

The function has the signature:

```
vector solve_pde(dt, Nx, K, T_meas, x_meas)
```

with arguments:

* `dt` - Timestep
* `Nx` - Number of interior points in spatial discretization
* `K` - Thermal diffusivity
* `T_meas` - Measurement time
* `x_meas` - Measurement points

Assume a true thermal diffusivity $K_{true} = 0.05$ and that we measure the
temperature in the rod at `Nx` points evenly spaced on the rod. We will
generate data under these conditions and try to recover the diffusivity later.

First, let's set up constants and plot a possible solution:

```{r diffusion, fig.width = 6.75, fig.height = 5}
dt = 1e-1
Nx = 5
L = 1.0

# Pretend we're measuring everywhere so we see the full solution
x = seq(-0.1, 1.1, length = 100)
x_meas = seq(0.0, L, length = 7)[2:6]
T_meas = 1.0
K_true = 0.025

u0 = solve_pde(dt, Nx, K_true, 0.0, x)
uT = solve_pde(dt, Nx, K_true, T_meas, x)
mu = solve_pde(dt, Nx, K_true, T_meas, x_meas)

plot(x, u0, type = 'l')
lines(x, uT, col = 'firebrick')
points(x_meas, mu, col = '#1864de')
```

The blue points in the solution are the ones we compare to (these
are the parts of the solution we care most about). *TODO: this figure probably needs more explaining.*

## Reliability of the solution

Our first question will be: is this solution accurate enough at the blue points? The simple way to check this is by computing the solution at the blue points at a higher space/time resolution and checking the difference.

We define a function that we can use for determining reliability, given a
fixed parameter value $K$. In our approach we double the space and time resolution and check the maximum absolute error.

```{r}
# Function to help determine if dt and Nx are small enough at given K
check_reliability = function(dt, Nx, K, T_meas, x_meas) {
  mu = solve_pde(dt, Nx, K, T_meas, x_meas)
  mu_more_accurate = solve_pde(dt / 2.0, 2 * Nx, K, T_meas, x_meas)
  max_abs_err = max(abs(mu_more_accurate - mu))
  return(max_abs_err)
}

# Check at K = K_true
check_reliability(dt, Nx, K_true, T_meas, x_meas)
```

Is that good or is that bad? That is something that will need to be determined in the context of the application. In this case I am going to generate data with a standard deviation of $0.1$, and so I want to get my numerical error quite a
bit below that before I am comfortable.

```{r}
dt = 0.01
Nx = 40
check_reliability(dt, Nx, K_true, T_meas, x_meas)
```

This seems good enough for now, but you might further refine your solution. Now
to simulate data:

```{r}
sigma = 0.1
noise = rnorm(length(x_meas), 0, sigma)
y = solve_pde(dt, Nx, K_true, T_meas, x_meas) + noise
```

## Applying the workflow

### Generating draws from $p_{low}$

We now take off our data-generating hats and put on our model-fitting hats. We fit an initial approximate model to our data. All our calculations are
approximations, and so we refer to this model as a low resolution model, because
we will check the calculations we did with it in the following steps.

Pretend `x_meas` is measured data and we do not know the true thermal diffusivity. Perhaps we are greedy and want this computation to finish quickly and so we fit this with a very aggressive approximation (one timestep, one spatial point):

```{r}
dt_low = 1.0
Nx_low = 1
fit = rstan::sampling(model,
                data = list(dt = dt_low,
                            Nx = Nx_low,
                            N_meas = length(x_meas),
                            T_meas = T_meas,
                            x_meas = x_meas,
                            y = y),
                cores = 4)
```

Looking at the posterior it appears everything was successful and the chains
mixed. However, we remember from earlier that $K_{true} = 0.1$, so something is
off. We will diagnose this using our approximation tools though.

```{r}
print(fit, pars = c("K", "sigma"))
```

### Tuning the reference method

We tune the high resolution method the same way as we did when generating the data, except that now we need to check the maximum absolute error (MAE) for all
parameter values that we sampled.

```{r}
dt_high = 0.005
Nx_high = 80

K_draws = rstan::extract(fit, "K")$K
sigma_draws = rstan::extract(fit, "sigma")$sigma
num_draws = length(K_draws)

# Compute differences
errors = c()
for (i in 1:num_draws) {
  mae = check_reliability(dt_high, Nx_high, K_draws[i], T_meas, x_meas)
  errors = c(errors, mae)
}
```

With a simple one parameter model we can plot our approximate errors as a
function of K (so we know the solution is suitable everywhere).

```{r}
error_plot = function(K, mae) {
  ylab = "Max. absolute error"
  plot(K, mae, pch = 4, col = "#1864de", xlab = "K", ylab = ylab)
}
error_plot(K_draws, errors)
```

The errors here seem low enough.

### Computing importance weights

The importance weights $\frac{p_{high}}{p_{low}}$  are computed on the log
scale. The priors cancel out so we only need to work with log likelihoods.

```{r}
# Define a function
compute_log_weights = function(
  dt_low, Nx_low, dt_high, Nx_high, 
  K_draws, T_meas, x_meas, y_meas)
{
  log_lh_low = rep(0, num_draws)
  log_lh_high = rep(0, num_draws)
  for (i in seq_len(num_draws)) {
    mu_low = solve_pde(dt_low, Nx_low, K_draws[i], T_meas, x_meas)
    mu_high = solve_pde(dt_high, Nx_high, K_draws[i], T_meas, x_meas)
    log_lh_low[i] = sum(dnorm(y_meas, mu_low, sigma_draws[i], log = TRUE))
    log_lh_high[i] = sum(dnorm(y_meas, mu_high, sigma_draws[i], log = TRUE))
  }
  log_weights = log_lh_high - log_lh_low
  return(log_weights)
}

# Apply function
log_weights = compute_log_weights(dt_low, Nx_low, dt_high, Nx_high,
                                  K_draws, T_meas, x_meas, y)
```

### Computing $\hat{k}$ diagnostic

If the $\hat{k}$ diagnostic is not low enough, it is not possible to do the
importance sampling correction and we need to recompute our posterior with a
higher resolution model. The `loo` package computes the $\hat{k}$ diagnostic for us:

```{r pareto, fig.width=6, fig.height=4.5}
loo::psis(log_weights)
```

Oh no! $\hat{k} > 0.5$, and it turns out modeling this process with one timestep
and one spatial point is not a good idea. This means we need to up the precision in the low resolution model and go back to *Step 1*.

### Repeating the loop

```{r}
dt_low = 0.1
Nx_low = 10
fit = rstan::sampling(model,
                data = list(dt = dt_low,
                            Nx = Nx_low,
                            N_meas = length(x_meas),
                            T_meas = T_meas,
                            x_meas = x_meas,
                            y = y),
                cores = 4)
```

Again, we can check our regular diagnostics:

```{r}
print(fit, pars = c("K", "sigma"))
```

Again, we verify our reference solution:

```{r}
dt_high = 0.005
Nx_high = 80
K_draws = rstan::extract(fit, "K")$K
sigma_draws = rstan::extract(fit, "sigma")$sigma
num_draws = length(K_draws)

# Compute differences
errors = c()
for (i in 1:num_draws) {
  mae = check_reliability(dt_high, Nx_high, K_draws[i], T_meas, x_meas)
  errors = c(errors, mae)
}

# Plot
error_plot(K_draws, errors)
```

And again we can compute the importance ratios and run the PSIS diagnostics on
them:

```{r}
log_weights = compute_log_weights(dt_low, Nx_low, dt_high, Nx_high,
                                  K_draws, T_meas, x_meas, y)
loo::psis(log_weights)
```

And this time $\hat{k} < 0.5$, so we are good enough!

### Resampling draws

At this point we have a weighted set of posterior draws. Again it
is usually easier to work with a set of draws than a set of weighted draws, so
we resample our weighted draws using `posterior::resample_draws`.

```{r}
w = exp(log_weights)
draws_list = rstan::extract(fit, c("K", "sigma"))
draws_df = posterior::as_draws_df(draws_list)
resampled_df = posterior::resample_draws(draws_df,
                                         weights = w)

print(draws_df %>% posterior::summarize_draws())
print(resampled_df %>% posterior::summarize_draws())
```

<!-- WEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE -->

# Computation environment

```{r session}
sessionInfo()
```

# References
