---
title: "Fast and reliable use of numerical solvers"
date: "`r Sys.Date()`"
author: "Juho Timonen and Ben Bales" 
output:
  html_document:
    toc: true
    theme: yeti
    highlight: textmate
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load, message=FALSE}
require(rstan)
library(tidyverse)
require(bayesplot)
require(grDevices)
require(ggplot2)
require(loo)
require(stats)
```

## 1. Introduction

Assume we have a Bayesian model that involves a numeric solution to some
ODE or PDE. Try as we might, these calculations are often expensive and
frequently becomes the limiting factors in whatever model they are involved in.

The simplest things we might do to speed up our calculations are lower the
timestep, coarsen the discretization, or increase the tolerance of the solvers.

That immediately leaves us with the question, is this okay? Has lowering the
tolerance affected the statistics of our solution? The technical answer is yes,
the practical answer is maybe, and the ideal case is no, but without checking
there's no way of knowing.

And so how can something like this be checked? The numerical approximation
answer is we should compare to a reference model and verify that the solution
at all points in space/time in the approximation are appropriately close
to the reference. That isn't so much of a problem in and of itself, but we're
doing statistics, and so we need to know that the solution is accurate enough
across all relevant parts of parameter space. Additionally, the "relevant parts
of parameter space" is the posterior itself which is not known beforehand!

And so the problem of validating an approximation in a Bayesian model is
significantly more complicated than in the classical numerical analysis world.
The point of this case study is to show how by adding one additional tool 
Pareto-Smoothed Importance Sampling (PSIS [@yao2018], [@vehtari2019]), we can
solve this problem.

## 2. PSIS and computing a reference model

To understand the usefulness of PSIS, we must first discuss importance sampling.
Assume we have two models, $p_{high}$ and $p_{low}$. $p_{high}$ will be an
expensive, high precision model and $p_{low}$ is a cheap, low precision model.
If we want to compute expectations with the high precision model, we can take
draws from the low precision models and reweight these according to the
importance weights $\frac{p_{high}}{p_{low}}$. If these models are too
different, then the reweighting will produce noisy estimates that are not
useful. PSIS and particularly the Pareto-k diagnostic is the tool that tells us
when we can or cannot rely on the importance weights.

Because of this, if $p_{high}$ is a reference model that we trust completely,
our workflow can be:

1. Generate draws from $p_{low}$
2. Compute importance weights $\frac{p_{high}}{p_{low}}$
3. Break if Pareto-k is sufficiently low, otherwise raise precision of $p_{low}$
and loop
4. Resample draws using importance weights

As described above though, we never really have a reference $p_{high}$ that
we trust completely because we never really know where exactly to evaluate the
solution. PSIS gives us an answer to this though. Expanding the workflow above
with a new step we get:

1. Generate draws from $p_{low}$
2. Compute a reference solution, $p_{high}$ at these draws
3. Compute importance weights $\frac{p_{high}}{p_{low}}$
4. Break if Pareto-k is sufficiently low, otherwise raise precision of $p_{low}$
and loop
5. Resample draws using importance weights

Step #2 should be done using whatever application and method specific numerical
techniques are applicable. In this workflow we have both solved the problem
of picking an appropriate approximation and also the problem of developing a
good reference model.

The rest of this case study is demonstrating how to do this on a simple PDE
model using Stan in combination with the [`loo`](https://mc-stan.org/loo) and
[`posterior`](https://github.com/stan-dev/posterior) software packages.

## 2. SIR example

I thought we could have an ODE example here, which doesn't require coding
a custom solver, just playing aroung with the tolerances.

This example is also studied in an earlier [case study](https://mc-stan.org/users/documentation/case-studies/boarding_school_case_study.html#1_simple_sir).

The [Stan documentation](https://mc-stan.org/docs/2_24/stan-users-guide/control-parameters-for-ode-solving.html) says 
*"The tolerances should be small enough so that setting them lower does not change the statistical properties of posterior samples generated by the Stan program but large enough to avoid unnecessary computation."*

The question is how to check if lowering doesn't change the statistical properties?

Wait, is `integrate_ode_bdf` deprecated in 2.24? Should we the `ode_bdf_tol` version?

## 3. 1D Diffusion Example

### 3.1  Problem definition

In this example we consider the diffusion of heat ($u(t, x)$) in a rod
($x \in [0, L]$). In this hypothetical experiment, the rod is cooled to room
temperature and then heated from both sides. After some time the temperature
profile of the rod is measured and from this a thermal diffusivity is to be
estimated.

The dynamics are governed by the 1D heat equation:

\begin{align}
\frac{\partial u}{\partial t} &= K \cdot \frac{\partial^2 u}{\partial x^2} \\
u(0, x) &= 0 \\
u(t, 0) &= 1 \\
u(t, L) &= 1
\end{align}

All of the computations in this case study are going to be done with a method of
lines discretization of this problem and a backwards Euler integrator. The
appropriate math is described in the online lecture notes
[ATM 623: Climate Modeling](http://www.atmos.albany.edu/facstaff/brose/classes/ATM623_Spring2015/Notes/Lectures/Lecture16%20--%20Numerical%20methods%20for%20diffusion%20models.html) by Brian E. J. Rose, though any introductory PDE reference should suffice.

For convenience we have defined a Stan function that solve our problem
given a timestep, a spatial discretization, a hypothetical diffusivity, a
measurement time, and a list of measurement points and returns the predicted
temperature at each of those measurement points.

```{r}
model <- stan_model("diffusion/diffusion.stan")
expose_stan_functions(model)
```

```{r}
dt = 1.0
Nx = 10
K = 1e-1
T_meas = 0.1
x_meas = c(-1.0, 0.01, 0.5, 0.99, 1.0, 2.0)

solve_pde(dt, Nx, K, T_meas, x_meas)
```

The function has the signature:

```
vector solve_pde(dt, Nx, K, T_meas, x_meas)
```

with arguments:

* `dt` - Timestep
* `Nx` - Number of interior points in spatial discretization
* `K` - Thermal diffusivity
* `T_meas` - Measurement time
* `x_meas` - Measurement points

Assume a true thermal diffusivity $K_{true} = 0.05$ and that we measure the
temperature in the rod at
$x_{meas} = \left [ \frac{1}{4} L, \frac{1}{2} L, \frac{3}{4} L \right ]$. We
will generate data under these conditions and try to recover the diffusivity
later.

First, let's set up constants and plot a reference solution:

```{r diffusion, fig.width = 6.75, fig.height = 5}
dt = 1e-1
Nx = 5
L = 1.0
# Pretend we're measuring everywhere so we see the full solution
x <- seq(-0.1, 1.1, length = 100)
x_meas <- seq(0.0, L, length = 12)[2:11]
T_meas <- 1.0
Ktrue <- 0.05

u0 = solve_pde(dt, Nx, Ktrue, 0.0, x)
uT = solve_pde(dt, Nx, Ktrue, T_meas, x)

y_low_res = solve_pde(dt, Nx, Ktrue, T_meas, x_meas)

plot(x, u0, type = 'l')
lines(x, uT, col = 'red')
points(x_meas, y_low_res, col = 'blue')
```

The blue points in the solution are the ones we compare to experiment (these
are the parts of the solution we care most about). Our first question will be:
is this solution accurate enough at the blue points? The simple way to check
this is by computing the solution at the blue points at a higher higher
space/time resolution and checking the difference.

Let's double the space and time resolution:

```{r}
y_high_res = solve_pde(dt / 2.0, 2 * Nx, Ktrue, T_meas, x_meas)
print(max(abs(y_high_res - y_low_res)))
```

So the maximum error is ```0.03```. Is that good or is that bad? That is
something that will need determined in the context of the application. In this
case I am going to generate data with a standard deviation of $0.01$, and so I
want to get my numerical error quite a bit below that before I am comfortable.

```{r}
dt = 0.01
Nx = 40
y_low_res = solve_pde(dt, Nx, Ktrue, T_meas, x_meas)
y_high_res = solve_pde(dt / 2.0, Nx * 2, Ktrue, T_meas, x_meas)
print(max(abs(y_high_res - y_low_res)))
```

This seems good enough for now, but you might further refine your solution. Now
to simulate data:

```{r}
sigma = 0.1
y = solve_pde(dt, Nx, Ktrue, T_meas, x_meas) + rnorm(length(x_meas), 0, sigma)
```

## Inference begins


```{r}
# Plot both
U <- rbind(u_0, u_T)
t <- c(0, T_max)
cols <- c("gray30", "firebrick3")
main <- paste0("Solution with dt=", dt, ", dx=", dx, ", K=", K_true)
plot_u(x_grid, U, t, cols, main)
```

## Fitting $p_{low}$

So now we have data, we can conveniently forget that we were the ones that
generated this data and we can go about fitting it.

Pretend we do not know the time and space resolution of the solver used to
generate the data. Perhaps we are greedy and start to fit this with a coarse
discretization:

```{r}
dt = 0.1
Nx = 5

fit <- sampling(model,
            data = list(dt = dt,
                        Nx = Nx,
                        N_meas = length(x_meas),
                        T_meas = T_meas,
                        x_meas = x_meas,
                        y = y),
            cores = 4)
```

We generate data $y$ by adding noise to $u(t=T,x)$.

```{r simulate, fig.width = 6.5, fig.height = 5}
sigma <- 0.15
y <- rnorm(length(u_T), u_T, sigma)
main <- paste0('Noisy observations of u(t,x) at t = ', T_max) 
plot(x_grid, y, pch = 20, main = main, xlab = 'x')
lines(x_grid, u_T, col = 'firebrick3', lwd = 2)
legend(0.1, 3.65, legend = c("y", "exp(u)"), lty = c(NA, 1), pch = c(20, NA),
       col = c('black', 'firebrick3'), lwd = c(2, 2))
```

### 3.4 Posterior inference in Stan

We define stan model and data.
```{r model, results=FALSE, message=FALSE}
stan_data <- list(N = length(x_grid),
                  x = x_grid,
                  y = y,
                  L = L,
                  T = T_max,
                  dt = dt,
                  sigma = sigma,
                  ul = ul,
                  ur = ur)
```
Hello, yes now we fit.

```{r fit}
fit <- sampling(model,
                data = stan_data,
                cores = 4)
```

We plot the draws of $K$.

```{r trace_plot, fig.width = 6.5, fig.height = 3.5}
bayesplot::mcmc_trace(fit, pars = 'K')
get_elapsed_time(fit)
```

Next we extract the computed solutions and show them.
```{r plot_fits, fig.width = 6.5, fig.height = 5}
U_draws <- rstan::extract(fit)$u
num_draws <- nrow(U_draws)
my_col <- grDevices::rgb(220, 20, 25, max = 255, alpha = 10)
plot(x_grid, y, pch = 20, main = "Model fit")
for (j in seq_len(num_draws)) {
  lines(x_grid, U_draws[j,], col = my_col)
}
```


### 3.5 Is the method accurate enough?

But how do we know if the method is accurate enough?

Assume we have a distribution $p_{true}(\theta)$ and $p_{approx}(\theta)$.

$p_{approx}$ is meant to approximate $p_{true}$. If they have the same support
then mathematically we can write expectations over $p_{true}$ as weighted
expectations over $p_{approx}$.

$$
E[f(\theta)] = \int f(\theta) p_{true}(\theta)\\
 = \int f(\theta) \frac{p_{true}(\theta)}{p_{approx}(\theta)}p_{approx}(\theta)d\theta
$$

If we're computing estimates with Monte Carlo, this means we replace
expectations over samples from our true distribution with weighted samples from
the approximate distribution.

$$
E[f(\theta)] \approx \frac{1}{N} \sum_n^N f(\theta_n) \\
\approx \frac{1}{N} \sum_n^N f(\theta_n) \frac{p_{true}(\theta)}{p_{approx}(\theta)}
$$
This is useful in MCMC when it is easier to sample from the approximation
than the truth. The ratios $\frac{p_{true}}{p_{approx}}$ are called importance
ratios. Perhaps unsurprisingly, this correction does not always work with
Monte Carlo. Indeed, if the difference in $p_{true}$ and $p_{approx}$ is too
great, then the importance ratios will be mostly things going to infinity or
zero.

In general, we can diagnose if the expectations will work by examing the
distribution of the importance weights by using the Pareto Smooth Importance
Sampling diagnostic. 

### 3.6 Computing reference solutions
For each draw of $K$, we compute a more reliable reference solution using 100x denser time discretization.

```{r reference, fig.width = 6.5, fig.height = 5}
K_draws <- rstan::extract(fit)$K
sigma_draws <- rstan::extract(fit)$sigma
U <- rstan::extract(fit)$u
U_ref <- 0.0 * U
num_draws <- nrow(U)
dt_ref <- dt / 100.0

start_time <- Sys.time()
for (i in seq_len(num_draws)) {
  U_ref[i,] <- stan_be(u_0, dt_ref, dx, T_max, K_draws[i], ul, ur)
}
Sys.time() - start_time
```
Now we could study the differences `U - U_ref`, but more that the accuracy of the solutions, we are interested in accuracy in terms of the likelihood.

```{r likelihoods}
log_lh <- rep(0, num_draws)
log_lh_ref <- rep(0, num_draws)
for (i in seq_len(num_draws)) {
  u <- U[i,]
  u_ref <- U_ref[i,]
  log_lh[i] <- sum(dnorm(y, u, sigma_draws[i], log = TRUE))
  log_lh_ref[i] <- sum(dnorm(y, u_ref, sigma_draws[i], log = TRUE))
}
```

They look the same
```{r plot1, fig.width=6.5, fig.height=4.5}
hist(log_lh_ref - log_lh)

# maybe use a color scale that is like red for negative differences and blue for positive?
df <- data.frame(log_lh, log_lh_ref, K_draws)
colnames(df)[3] <- "K"
plt <- ggplot(df, aes_string(x = "log_lh", y = "log_lh_ref", col = "K")) + geom_point()
A <- min(c(log_lh, log_lh_ref))
B <- max(c(log_lh, log_lh_ref))
df_line <- data.frame(x = c(A,B), y = c(A,B))
plt <- plt + geom_line(data = df_line, aes_string(x = "x", y = "y"), inherit.aes = FALSE, col = "firebrick3", lty = 2)
plt
```

```{r plot2, fig.width=6.5, fig.height=4.5}
df <- data.frame(K_draws, sigma_draws, log_lh - log_lh_ref)
colnames(df) <- c("K", "sigma", "diff")
plt <- ggplot(df, aes_string(x = "K", y = "sigma", col = "diff")) + geom_point()
plt
```

Pareto-k diagnostic
```{r pareto, fig.width=6, fig.height=4.5}
loo::psis(log_lh - log_lh_ref)$diagnostics
```

### 3.7 Correcting the Approximation
Todo.

## Computation environment

```{r session}
sessionInfo()
```

## References
