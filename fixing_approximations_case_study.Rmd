---
title: "Fixing Approximations"
date: "10/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(cmdstanr)
```

## Fixing Approximations

Assume we have a Bayesian model:

$p(x | \theta) p(\theta)$

Try as we might to make this model go fast, sometimes it just doesn't. In fact,
often it doesn't.

There are numerous reasons this might happen:
1. Difficult to sample posterior geometry
2. Bug in the code
3. Model does not fit the data well
4. Lots of data
5. etc...

The particular situation we want to discuss here is one where you are happy
with your model but there are some difficult calculations you want to
approximate for performance reasons.

This comes up relatively often when dealing with integrals and differential
equations in Stan. Stan comes with a couple common ODE integrators and one
quadrature scheme in the language itself, but if those prepackaged solutions
do not work, something else must be done. Usually what happens is an approximate
quadrature or integrator gets used that is much faster or more stable than the
reference implementation in Stan. The question immediately becomes, how do
I know my approximation is good enough?

The goal of this case study is to figure out if your approximation is good
enough, and, if possible use importance sampling to correct for any bias in
the results. This is using techniques from the paper "Yes, But Did it Work?" ([arXiv:1802.02538](https://arxiv.org/abs/1802.02538)), "Pareto Smoothed
Importance Sampling" ([arXiv:1507.02646](https://arxiv.org/abs/1507.02646)),
and the `loo` software package (https://mc-stan.org/loo).

## 1D Diffusion Example

First, let's start with a problem where Stan is slow.

Let's pretend that we are modeling a 1D diffusion problem on $x \in [0, L]$ with
initial concentration that is a step function at $x = L / 2$ and with fixed
boundary conditions (that match the step). So that means:

$$
\frac{\partial u}{\partial t} = D \frac{\partial^2 u}{\partial x^2} \\
u(t, x = 0) = 0 \\
u(t, x = L) = 1 \\
u(t = 0, x) = x > \frac{L}{2}
$$

Let's pretend that we're going to measure the concentration of this system at
time $t = T$ and try to estimate the diffusion constant.

Let's assume $D_{true} = 0.5$. This is what we are going to try to estimate.
We can easily discretize this using method of lines and generate some sample
data in R:

```{r}
L = 1
Tf = 1
D = 0.25
dx = 1e-2
dt = 1e-4

x = seq(0, 1, by = dx)
u_init = x > L / 2.0

u = u_init
for(t in seq(0, Tf, by = dt)) {
  u_new = u
  
  for(i in 2:(length(u) - 1)) {
    u_new[i] = dt * (u[i + 1] - 2 * u[i] + u[i - 1]) / (2 * dx) + u[i]
  }
  
  u = u_new
}

plot(x, u_init, type = 'l')
lines(x, u, col = 'red')
```

Now, if you're familiar with diffusion equations you might say, 'Oh, you
fools, you've done a forward Euler method on a diffusion problem and you're
not even trying to control your error due to your timestep or your spatial
discretization.'

Since you are this far in the case study, we will admit, yes, this is a bad way
to solve the problem. We did it in a pinch because it was easier to do it
this way than worry about an adaptive timestep/adaptive meshsize method.
After all, we are solving a 1D diffusion. We can just make a finer
discretization or use smaller timesteps and see how the error between successive
approximations gets smaller.

But how should we check this in a model in Stan? The error in the solution from
using an approximation is important, but really the error in the posterior is
what matters. That leads to our first question

## Is the Approximation Good Enough?

Assume we have a distribution $p_{true}(\theta)$ and $p_{approx}(\theta)$.

$p_{approx}$ is meant to approximate $p_{true}$. If they have the same support
then mathematically we can write expectations over $p_{true}$ as weighted
expectations over $p_{approx}$.

$$
E[f(\theta)] = \int f(\theta) p_{true}(\theta)\\
 = \int f(\theta) \frac{p_{true}(\theta)}{p_{approx}(\theta)}p_{approx}(\theta)d\theta
$$

If we're computing estimates with Monte Carlo, this means we replace
expectations over samples from our true distribution with weighted samples from
the approximate distribution.

$$
E[f(\theta)] \approx \frac{1}{N} \sum_n^N f(\theta_n) \\
\approx \frac{1}{N} \sum_n^N f(\theta_n) \frac{p_{true}(\theta)}{p_{approx}(\theta)}
$$
This is useful in MCMC when it is easier to sample from the approximation
than the truth. The ratios $\frac{p_{true}}{p_{approx}} are called importance
ratios. Perhaps unsurprisingly, this correction does not always work with
Monte Carlo. Indeed, if the difference in $p_{true}$ and $p_{approx}$ is too
great, then the importance ratios will be mostly things going to infinity or
zero.

In general, we can diagnose if the expectations will work by examing the
distribution of the importance weights by using the Pareto Smooth Importance
Sampling diagnostic. But first, let's write a Stan model

## 1D Diffusion in Stan

```{r}
model = cmdstan_model("diffusion.stan")

fit = model$sample(data = list(N = length(x),
                               x = x,
                               y = rnorm(y, 0.1),
                               L = L,
                               Tf = 0.1,
                               dt = 1e-3,
                               sigma = 0.1),
                   iter_warmup = 1000,
                   iter_sampling = 1000,
                   chains = 4,
                   cores = 4)
```


## Correcting the Approximation
